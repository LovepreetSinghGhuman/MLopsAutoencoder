{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📘 IEEE-CIS Fraud Detection with Autoencoders\n",
    "---\n",
    "**Objective**: Detect fraudulent transactions using anomaly detection with autoencoders.  \n",
    "This notebook implements an anomaly detection pipeline using an autoencoder neural network. Below is an overview of the key steps and components:\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ 1. **Import Required Libraries**\n",
    "- **Standard Libraries:** For system operations, logging, time management, and type annotations.\n",
    "- **Data Processing & Visualization:** Uses libraries such as `pandas`, `numpy`, `matplotlib`, `seaborn`, and `joblib` for data manipulation and visualizations.\n",
    "- **Sklearn Utilities:** Includes modules for data preprocessing, dataset splitting, and a wide range of evaluation metrics.\n",
    "- **Statistical Tools:** Utilizes variance inflation factor (VIF) from `statsmodels` to assess multicollinearity.\n",
    "- **Deep Learning with TensorFlow/Keras:** For building the autoencoder model, including layers, callbacks, and optimizers.\n",
    "- **Hyperparameter Tuning:** Implements Keras Tuner with `Hyperband` for optimized parameter selection.\n",
    "- **XGBoost:** Optionally included to support alternative modeling or feature analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 2. **Define Configuration Class**\n",
    "\n",
    "- The `Config` class centralizes all key parameters and directory paths for the project:\n",
    "  - **Reproducibility:** Sets a random seed for both NumPy and TensorFlow.\n",
    "  - **Model Training Parameters:** Specifies epochs, batch size, validation size, learning rate, threshold range, and the number of thresholds to search.\n",
    "  - **Early Stopping & Learning Rate Scheduling:** Configures patience, reduction factor, and minimum learning rate for callbacks.\n",
    "  - **Directory Structure:** Defines base, processed data, results, and model directories using `pathlib.Path` for cross-platform compatibility.\n",
    "  - **Model & Config File Paths:** Stores paths for the best model, threshold, and configuration files.\n",
    "  - **Directory Initialization:** The `prepare_dirs()` static method ensures all required directories exist before training or saving outputs.\n",
    "\n",
    "This design keeps all configuration in one place, reduces redundancy, and makes the pipeline easier to maintain and update.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 3. **Environment Setup**\n",
    "- **Directory Initialization:** Calls `Config.prepare_dirs()` to establish the required folder structure.\n",
    "- **Seed Setting:** Establishes seeds for both `numpy` and `TensorFlow` to ensure reproducible outcomes.\n",
    "- **Logging and Warnings:** Configures the logger for tracking progress, and suppresses warnings to streamline the output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: keras-tuner in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lovep\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lovep\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.72.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lovep\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lovep\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (4.14.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: optree in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: namex in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lovep\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lovep\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\lovep\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas matplotlib seaborn joblib scikit-learn statsmodels tensorflow keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lovep\\AppData\\Local\\Temp\\ipykernel_23720\\1821542819.py:38: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import HyperModel\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library and Data Science Imports ===\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# === Sklearn Imports ===\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# === TensorFlow / Keras Imports ===\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# === Keras Tuner Imports ===\n",
    "from kerastuner import HyperModel\n",
    "from keras_tuner import HyperModel, HyperParameters, Hyperband, Objective, RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config Class === \n",
    "class Config:\n",
    "    # General settings\n",
    "    SEED = 42\n",
    "    EPOCHS = 150\n",
    "    BATCH_SIZE = 256\n",
    "    VAL_SIZE = 0.2\n",
    "    LEARNING_RATE = 1e-4\n",
    "    OPTIMIZE_BY = \"roc_auc\"  # or \"f1_score\"\n",
    "    THRESHOLD_RANGE = (0.9, 0.99)\n",
    "    NUM_THRESHOLDS = 100\n",
    "\n",
    "    # Early stopping and LR scheduling\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    REDUCE_LR_PATIENCE = 5\n",
    "    REDUCE_LR_FACTOR = 0.5\n",
    "    MIN_LR = 1e-6\n",
    "\n",
    "    # Directory paths\n",
    "    BASE_DIR = Path(\".\")\n",
    "    DATA_DIR_PROCESS = BASE_DIR / \"Data\" / \"Processed\"\n",
    "    RESULTS_DIR = BASE_DIR / \"Results\"\n",
    "    MODEL_DIR = BASE_DIR / \"Models\"\n",
    "\n",
    "    # Model and config file paths\n",
    "    MODEL_PATH = MODEL_DIR / \"best_autoencoder.keras\"\n",
    "    THRESHOLD_PATH = MODEL_DIR / \"best_threshold.json\"\n",
    "    CONFIG_PATH = MODEL_DIR / \"autoencoder_config.json\"\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_dirs():\n",
    "        for directory in [\n",
    "            Config.DATA_DIR_PROCESS,\n",
    "            Config.MODEL_DIR,\n",
    "            Config.RESULTS_DIR,\n",
    "        ]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Environment Setup ===\n",
    "Config.prepare_dirs()\n",
    "np.random.seed(Config.SEED)\n",
    "tf.random.set_seed(Config.SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Data Loading, Cleaning, and Validation Steps\n",
    "\n",
    "This section prepares the data for modeling by loading, cleaning, aligning, and saving the datasets. It ensures both train and test data are consistent and ready for downstream processing.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Load Raw Data**\n",
    "\n",
    "- Loads the merged training and test datasets from the processed data directory.\n",
    "- Handles missing file errors with informative logging.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Clean Data**\n",
    "\n",
    "- Removes unnecessary columns such as `TransactionID` and index columns like `Unnamed: 0`.\n",
    "- Optionally drops columns with excessive missing values (less than 10% non-null).\n",
    "- Fills missing values with `np.nan` for further handling.\n",
    "- Applies the cleaning function to both train and test datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Column Alignment and Validation**\n",
    "\n",
    "- Prints the columns of both train and test datasets after cleaning for transparency.\n",
    "- Checks for any feature columns present in train but missing in test, ensuring feature consistency.\n",
    "- Validates that both datasets are aligned for subsequent preprocessing and modeling steps.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Save Cleaned Data**\n",
    "\n",
    "- Saves the cleaned train and test datasets as `cleaned_train.csv` and `cleaned_test.csv` in the processed data directory.\n",
    "- Confirms successful saving with a message.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "This workflow guarantees that both training and test data are consistently cleaned, aligned, and saved, providing a reliable foundation for feature engineering, scaling, and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_data = pd.read_csv(Config.DATA_DIR_PROCESS /'merged_train.csv')\n",
    "    test_data = pd.read_csv(Config.DATA_DIR_PROCESS/'merged_test.csv')\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Data files not found: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['isFraud']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any feature columns available in train but not in test\n",
    "different_features = [features for features in train_data.columns if features not in test_data.columns]\n",
    "different_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the dataset\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by:\n",
    "    - Removing duplicate rows\n",
    "    - Handling missing values (numeric: median, categorical: 'missing')\n",
    "    - Removing constant columns\n",
    "    - Removing columns with too many missing values (>90%)\n",
    "    - Mapping email domains to groups using EMAIL_DOMAIN_MAP\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Unify id feature names: id-XX -> id_XX\n",
    "    df.columns = df.columns.str.replace(r'^id-', 'id_', regex=True)\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Remove constant columns\n",
    "    nunique = df.nunique()\n",
    "    constant_cols = nunique[nunique <= 1].index\n",
    "    df = df.drop(columns=constant_cols)\n",
    "\n",
    "    # Remove columns with >90% missing values\n",
    "    missing_ratio = df.isnull().mean()\n",
    "    high_missing_cols = missing_ratio[missing_ratio > 0.9].index\n",
    "    df = df.drop(columns=high_missing_cols)\n",
    "\n",
    "    # Map email domains to groups\n",
    "    for col in ['P_emaildomain', 'R_emaildomain']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.lower().map(EMAIL_DOMAIN_MAP).fillna('other')\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna('missing')\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Efficient email domain mapping using dictionary and pandas .map()\n",
    "EMAIL_DOMAIN_MAP = {\n",
    "    # Google\n",
    "    'gmail.com': 'google', 'googlemail.com': 'google',\n",
    "    # Yahoo\n",
    "    'yahoo.com': 'yahoo', 'yahoo.com.mx': 'yahoo', 'yahoo.co.uk': 'yahoo', 'yahoo.co.jp': 'yahoo',\n",
    "    'ymail.com': 'yahoo', 'rocketmail.com': 'yahoo',\n",
    "    # Microsoft\n",
    "    'hotmail.com': 'microsoft', 'outlook.com': 'microsoft', 'live.com': 'microsoft', 'msn.com': 'microsoft',\n",
    "    # Apple\n",
    "    'icloud.com': 'apple', 'me.com': 'apple', 'mac.com': 'apple',\n",
    "    # AOL\n",
    "    'aol.com': 'aol', 'aim.com': 'aol',\n",
    "    # Protonmail\n",
    "    'protonmail.com': 'protonmail',\n",
    "    # Comcast\n",
    "    'comcast.net': 'comcast',\n",
    "    # Verizon\n",
    "    'verizon.net': 'verizon',\n",
    "    # Optonline\n",
    "    'optonline.net': 'optonline',\n",
    "    # Cox\n",
    "    'cox.net': 'cox',\n",
    "    # Charter\n",
    "    'charter.net': 'charter',\n",
    "    # AT&T\n",
    "    'att.net': 'att', 'sbcglobal.net': 'att', 'bellsouth.net': 'att',\n",
    "    # Earthlink\n",
    "    'earthlink.net': 'earthlink',\n",
    "    # Embarqmail\n",
    "    'embarqmail.com': 'embarqmail',\n",
    "    # Frontier\n",
    "    'frontier.com': 'frontier', 'frontiernet.net': 'frontier',\n",
    "    # Windstream\n",
    "    'windstream.net': 'windstream',\n",
    "    # Spectrum\n",
    "    'twc.com': 'spectrum', 'roadrunner.com': 'spectrum',\n",
    "    # Centurylink\n",
    "    'centurylink.net': 'centurylink',\n",
    "    # Suddenlink\n",
    "    'suddenlink.net': 'suddenlink',\n",
    "    # Netzero\n",
    "    'netzero.net': 'netzero', 'netzero.com': 'netzero',\n",
    "    # GMX\n",
    "    'gmx.de': 'gmx', 'gmx.com': 'gmx',\n",
    "    # Mail.ru\n",
    "    'mail.ru': 'mailru',\n",
    "    # Naver\n",
    "    'naver.com': 'naver',\n",
    "    # Yandex\n",
    "    'yandex.ru': 'yandex', 'yandex.com': 'yandex',\n",
    "    # Mail.com\n",
    "    'mail.com': 'mail.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to both train_data and test_data\n",
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned train and test data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned datasets\n",
    "train_data.to_csv(Config.DATA_DIR_PROCESS / 'cleaned_train.csv', index=False)\n",
    "test_data.to_csv(Config.DATA_DIR_PROCESS / 'cleaned_test.csv', index=False)\n",
    "\n",
    "print(\"Cleaned train and test data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Data Preprocessing and Train/Validation Split\n",
    "\n",
    "This section details a robust and reproducible workflow for preparing the IEEE-CIS fraud detection data for autoencoder modeling. It covers advanced feature engineering, careful scaling, and strategic splitting of the data to ensure fair evaluation and prevent data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Preprocessing Pipeline**\n",
    "\n",
    "- **Function:** `preprocess_data(df, scaler=None, is_train=True, scaler_path=\"scaler.joblib\", clip_values=True, feature_columns=None)`\n",
    "- **Key Steps:**\n",
    "  1. **Column Removal:** Drops `TransactionID` and `isFraud` to focus on features only.\n",
    "  2. **Feature Engineering:**\n",
    "     - If `TransactionDT` exists, extracts `hour`, `dayofweek`, and `dayofmonth` to capture temporal patterns.\n",
    "     - If `TransactionAmt` exists, applies a log transformation for normalization and removes extreme outliers in training.\n",
    "  3. **Missing Value Handling:** \n",
    "     - Fills missing values in categorical columns with `'NaN'`.\n",
    "     - Fills missing values in numeric columns with the median.\n",
    "  4. **Categorical Encoding:** Converts all categorical columns to integer codes for model compatibility.\n",
    "  5. **Winsorization:** Clips numeric columns to the 1st and 99th percentiles to reduce the impact of outliers.\n",
    "  6. **Numeric Filtering:** Retains only numeric columns and ensures all values are finite.\n",
    "  7. **Extreme Value Clipping:** Optionally clips all values to a safe range to prevent instability.\n",
    "  8. **Column Alignment:** Ensures test/validation data columns match the training set for consistency.\n",
    "  9. **Scaling:** \n",
    "     - Fits and saves a `StandardScaler` on training data.\n",
    "     - Loads and applies the scaler for test/validation data.\n",
    "- **Output:** Returns the scaled feature matrix, the fitted scaler, and the list of feature columns.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Train/Validation Splitting Strategy**\n",
    "\n",
    "- **Time-based Split:** \n",
    "  - Splits normal (non-fraud) transactions into training and validation sets based on the `TransactionDT` timestamp. This simulates a real-world scenario and prevents temporal leakage.\n",
    "- **Stratified Split:** \n",
    "  - Further splits the data for validation using stratification on the `isFraud` label to maintain the original class distribution in both sets.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Execution Flow**\n",
    "\n",
    "- **Step 1:** Perform a time-based split on normal transactions to create training and validation sets.\n",
    "- **Step 2:** Preprocess the training data, fit the scaler, and extract feature columns.\n",
    "- **Step 3:** Use a stratified split on the full dataset to create a final validation set for model evaluation.\n",
    "- **Step 4:** Preprocess the validation set using the saved scaler and align columns to the training set.\n",
    "- **Step 5:** Print the class distribution in the validation set to ensure proper stratification and transparency.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "This pipeline ensures that all preprocessing steps are applied consistently and reproducibly, with careful handling of feature engineering, scaling, and data splitting. The approach is designed to maximize model robustness and evaluation fairness, providing a solid foundation for downstream autoencoder training and fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Config.DATA_DIR_PROCESS / 'cleaned_train.csv')\n",
    "test_data = pd.read_csv(Config.DATA_DIR_PROCESS / 'cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    df,\n",
    "    scaler=None,\n",
    "    is_train=True,\n",
    "    scaler_path=Config.MODEL_DIR/\"scaler.joblib\",\n",
    "    clip_values=True,\n",
    "    feature_columns=None\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Drop ID and target columns if present\n",
    "    drop_cols = ['TransactionID', 'isFraud']\n",
    "    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True, errors='ignore')\n",
    "\n",
    "    # 2. Feature Engineering: TransactionDT\n",
    "    if 'TransactionDT' in df.columns:\n",
    "        START_DATE = pd.Timestamp('2017-11-30')\n",
    "        df['TransactionDT_datetime'] = START_DATE + pd.to_timedelta(df['TransactionDT'], unit='s')\n",
    "        df['hour'] = df['TransactionDT_datetime'].dt.hour\n",
    "        df['dayofweek'] = df['TransactionDT_datetime'].dt.dayofweek\n",
    "        df['dayofmonth'] = df['TransactionDT_datetime'].dt.day\n",
    "        df.drop(columns=['TransactionDT', 'TransactionDT_datetime'], inplace=True)\n",
    "\n",
    "    # 3. Feature Engineering: TransactionAmt (log transform, remove outliers in train)\n",
    "    if 'TransactionAmt' in df.columns:\n",
    "        if is_train:\n",
    "            df = df[df['TransactionAmt'] < 30000]\n",
    "        df['TransactionAmt_Log'] = np.log1p(np.clip(df['TransactionAmt'], a_min=0, a_max=None))\n",
    "        # TransactionAmt relative to card1 mean\n",
    "        if 'card1' in df.columns:\n",
    "            card1_mean = df.groupby('card1')['TransactionAmt'].transform('mean')\n",
    "            df['TransactionAmt_to_card1_mean'] = df['TransactionAmt'] / (card1_mean + 1e-3)\n",
    "        df.drop(columns=['TransactionAmt'], inplace=True)\n",
    "\n",
    "    # 4. DeviceInfo grouping (prioritize device type)\n",
    "    if 'DeviceInfo' in df.columns:\n",
    "        df['DeviceInfo'] = df['DeviceInfo'].fillna('missing').str.lower()\n",
    "        df['DeviceInfo_grouped'] = df['DeviceInfo'].apply(lambda x: x.split(' ')[0] if isinstance(x, str) else 'missing')\n",
    "\n",
    "    # 5. DeviceType (fill missing, encode)\n",
    "    if 'DeviceType' in df.columns:\n",
    "        df['DeviceType'] = df['DeviceType'].fillna('missing').str.lower()\n",
    "\n",
    "    # 6. Prioritize ProductCD, card1-card6, addr1/addr2, M1-M9, id_30, id_31, id_33, id_34, id_36, id_38\n",
    "    # Frequency encoding for high-cardinality features\n",
    "    for col in ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']:\n",
    "        if col in df.columns:\n",
    "            freq = df[col].value_counts(dropna=False)\n",
    "            df[col + '_freq'] = df[col].map(freq)\n",
    "            df[col] = df[col].fillna(-999)\n",
    "\n",
    "    # 7. Fill missing values for categorical/object columns\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].fillna('missing')\n",
    "\n",
    "    # 8. Fill missing values for numeric columns\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # 9. Encode categorical columns (robustly)\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "        df[col] = df[col].cat.codes\n",
    "\n",
    "    # 10. Winsorize numeric columns (clip to 1st/99th percentile for robustness)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        lower = df[col].quantile(0.01)\n",
    "        upper = df[col].quantile(0.99)\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "\n",
    "    # 11. Select numeric columns only\n",
    "    df = df.select_dtypes(include=[np.number]).astype(np.float32)\n",
    "\n",
    "    # 12. Replace non-finite values with 0\n",
    "    df[~np.isfinite(df)] = 0.0\n",
    "\n",
    "    # 13. Optionally clip extreme values\n",
    "    if clip_values:\n",
    "        df = df.clip(lower=-1e6, upper=1e6)\n",
    "\n",
    "    # 14. Ensure consistent columns between train and test\n",
    "    if feature_columns is not None:\n",
    "        df = df.reindex(columns=feature_columns, fill_value=0)\n",
    "    elif is_train:\n",
    "        feature_columns = df.columns.tolist()\n",
    "\n",
    "    # 15. Scaling\n",
    "    if scaler is None:\n",
    "        if is_train:\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(df)\n",
    "            joblib.dump(scaler, scaler_path)\n",
    "            X_scaled = pd.DataFrame(X_scaled, columns=df.columns, index=df.index)\n",
    "        else:\n",
    "            try:\n",
    "                scaler = joblib.load(scaler_path)\n",
    "                X_scaled = scaler.transform(df)\n",
    "                X_scaled = pd.DataFrame(X_scaled, columns=df.columns, index=df.index)\n",
    "            except FileNotFoundError:\n",
    "                raise ValueError(\"Scaler required for inference. Provide a fitted scaler or ensure scaler.joblib exists.\")\n",
    "    else:\n",
    "        X_scaled = scaler.transform(df)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=df.columns, index=df.index)\n",
    "\n",
    "    return X_scaled, scaler, feature_columns\n",
    "\n",
    "def stratified_split(data, val_ratio, seed):\n",
    "    \"\"\"\n",
    "    Performs a stratified split on the dataset based on the 'isFraud' column.\n",
    "    Stratified split means dividing your dataset into training and validation (or test) sets while preserving the proportion of each class label\n",
    "    \"\"\"\n",
    "    return train_test_split(\n",
    "        data,\n",
    "        test_size=val_ratio,\n",
    "        stratify=data[\"isFraud\"],\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "def split_normal_data(data, timestamp_col, val_ratio):\n",
    "    \"\"\"\n",
    "    Splits normal (non-fraud) data into training and validation sets based on time.\n",
    "    \"\"\"\n",
    "    normal_data = data[data[\"isFraud\"] == 0].sort_values(by=timestamp_col)\n",
    "    split_index = int(len(normal_data) * (1 - val_ratio))\n",
    "    return normal_data.iloc[:split_index], normal_data.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preprocessing ===\n",
    "# Step 1: Time-based split on normal (non-fraud) data\n",
    "X_train, X_val = split_normal_data(train_data, timestamp_col=\"TransactionDT\", val_ratio=Config.VAL_SIZE)\n",
    "\n",
    "# Step 2: Preprocess training data (fit scaler here)\n",
    "X_train_auto, scaler, feature_columns = preprocess_data(X_train, is_train=True)\n",
    "\n",
    "# Step 3: Stratified split for validation (for final evaluation)\n",
    "_, val_temp = stratified_split(train_data, val_ratio=Config.VAL_SIZE, seed=Config.SEED)\n",
    "\n",
    "# Step 4: Preprocess validation data using saved scaler\n",
    "X_val_auto, _, _ = preprocess_data(val_temp, scaler=scaler, is_train=False, feature_columns=feature_columns)\n",
    "y_val_auto = val_temp[\"isFraud\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Model Building, Training, and Evaluation Pipeline\n",
    "\n",
    "This section details the full workflow for constructing, training, and evaluating autoencoder models for fraud detection. It covers model architecture selection, training with callbacks, threshold optimization, and model persistence.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Autoencoder Model Construction**\n",
    "\n",
    "- **Function:** `build_autoencoder(input_dim: int, version: str)`\n",
    "- **Purpose:** Builds an autoencoder model with selectable architecture versions:\n",
    "  - **v1:** Basic encoder-decoder with dense layers and batch normalization.\n",
    "  - **v2:** Adds dropout and stronger L2 regularization for improved generalization.\n",
    "  - **v3:** Deeper model with more layers, dropout, and batch normalization.\n",
    "  - **v4:** Uses LeakyReLU activations, advanced initialization, and a deeper encoder/decoder.\n",
    "  - **v5:** Deepest model with an explicit bottleneck, heavy dropout, and batch normalization.\n",
    "- **Output:** Returns both the full autoencoder and the encoder (for feature extraction).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Model Training**\n",
    "\n",
    "- **Function:** `train_autoencoder(model, X_train, X_val=None, save_path=None)`\n",
    "- **Purpose:** Trains the autoencoder using MSE loss and Adam optimizer.\n",
    "- **Callbacks:**\n",
    "  - **EarlyStopping:** Stops training if validation loss does not improve.\n",
    "  - **ReduceLROnPlateau:** Reduces learning rate on plateau.\n",
    "  - **ModelCheckpoint:** Saves the best model during training.\n",
    "- **Output:** Returns the training history for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Reconstruction Error Calculation**\n",
    "\n",
    "- **Function:** `get_reconstruction_errors(model, X)`\n",
    "- **Purpose:** Computes the mean squared error between input and reconstruction for each sample, used for anomaly scoring.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Model Evaluation**\n",
    "\n",
    "- **Function:** `evaluate_with_threshold(model, X, y, threshold)`\n",
    "- **Purpose:** Evaluates model performance by applying a threshold to reconstruction errors to classify anomalies.\n",
    "- **Metrics:** ROC-AUC, confusion matrix, classification report, average precision, F1, precision, and recall.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Threshold Optimization**\n",
    "\n",
    "- **Function:** `select_best_threshold(model, X, y, threshold_range, n_jobs, optimize_by)`\n",
    "- **Purpose:** Searches for the optimal threshold over a quantile range of reconstruction errors, maximizing a chosen metric (e.g., ROC-AUC or F1).\n",
    "- **Parallelization:** Evaluates thresholds in parallel for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Training and Model Selection Workflow**\n",
    "\n",
    "- **Steps:**\n",
    "  1. For each model version (`v1`–`v5`):\n",
    "     - Build and train the autoencoder.\n",
    "     - Optimize the threshold using validation data.\n",
    "     - Evaluate and log performance metrics.\n",
    "     - Save the best-performing model, threshold, and configuration.\n",
    "  2. After all versions are trained, the best model (by ROC-AUC) is selected and persisted.\n",
    "- **Visualization:** Plots training and validation loss curves for all model versions.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "This pipeline automates the process of building, training, validating, and saving the best autoencoder model and its threshold for robust fraud detection, ensuring reproducibility and optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Autoencoder Model Builder ---\n",
    "def build_autoencoder(input_dim: int, version: str = \"v1\") -> Tuple[Model, Model]:\n",
    "    \"\"\"\n",
    "    Architecture version to use. Options:\n",
    "        - \"v1\": Basic encoder-decoder with dense layers and batch normalization.\n",
    "        - \"v2\": Adds dropout and stronger L2 regularization for improved generalization.\n",
    "        - \"v3\": Deeper model with more layers, dropout, and batch normalization.\n",
    "        - \"v4\": Uses LeakyReLU activations, advanced initialization, and a deeper encoder/decoder.\n",
    "        - \"v5\": Deepest model with an explicit bottleneck, heavy dropout, and batch normalization.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(input_dim,), name=\"input_layer\")\n",
    "    \n",
    "    if version == \"v1\":\n",
    "        # Basic version: 128 -> 64 in encoder; symmetric decoder\n",
    "        encoded = Dense(128, activation=\"relu\", activity_regularizer=l2(1e-4), name=\"encoder_dense_1\")(input_layer)\n",
    "        encoded = BatchNormalization(name=\"encoder_bn_1\")(encoded)\n",
    "        encoded = Dense(64, activation=\"relu\", name=\"encoder_dense_2\")(encoded)\n",
    "        \n",
    "        decoded = Dense(64, activation=\"relu\", name=\"decoder_dense_1\")(encoded)\n",
    "        decoded = Dense(128, activation=\"relu\", name=\"decoder_dense_2\")(decoded)\n",
    "        decoded = Dense(input_dim, activation=\"linear\", name=\"output_layer\")(decoded)\n",
    "        \n",
    "    elif version == \"v2\":\n",
    "        # Adds dropout and stronger regularization\n",
    "        encoded = Dense(256, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_1\")(input_layer)\n",
    "        encoded = BatchNormalization(name=\"encoder_bn_1\")(encoded)\n",
    "        encoded = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_2\")(encoded)\n",
    "        encoded = Dropout(0.2, name=\"encoder_dropout_1\")(encoded)\n",
    "        encoded = Dense(64, activation=\"relu\", name=\"encoder_dense_3\")(encoded)\n",
    "        \n",
    "        decoded = Dense(64, activation=\"relu\", name=\"decoder_dense_1\")(encoded)\n",
    "        decoded = Dropout(0.2, name=\"decoder_dropout_1\")(decoded)\n",
    "        decoded = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_2\")(decoded)\n",
    "        decoded = BatchNormalization(name=\"decoder_bn_1\")(decoded)\n",
    "        decoded = Dense(256, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_3\")(decoded)\n",
    "        decoded = Dense(input_dim, activation=\"linear\", name=\"output_layer\")(decoded)\n",
    "        \n",
    "    elif version == \"v3\":\n",
    "        # Improved version of v2 with deeper layers, advanced activation, and better regularization\n",
    "        encoded = Dense(512, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_1\")(input_layer)\n",
    "        encoded = BatchNormalization(name=\"encoder_bn_1\")(encoded)\n",
    "        encoded = Dense(256, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_2\")(encoded)\n",
    "        encoded = Dropout(0.3, name=\"encoder_dropout_1\")(encoded)\n",
    "        encoded = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_3\")(encoded)\n",
    "        encoded = BatchNormalization(name=\"encoder_bn_2\")(encoded)\n",
    "        encoded = Dense(64, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_4\")(encoded)\n",
    "\n",
    "        decoded = Dense(64, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_1\")(encoded)\n",
    "        decoded = Dropout(0.3, name=\"decoder_dropout_1\")(decoded)\n",
    "        decoded = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_2\")(decoded)\n",
    "        decoded = BatchNormalization(name=\"decoder_bn_1\")(decoded)\n",
    "        decoded = Dense(256, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_3\")(decoded)\n",
    "        decoded = Dropout(0.3, name=\"decoder_dropout_2\")(decoded)\n",
    "        decoded = Dense(512, activation=\"relu\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_4\")(decoded)\n",
    "        decoded = Dense(input_dim, activation=\"linear\", name=\"output_layer\")(decoded)\n",
    "        \n",
    "    elif version == \"v4\":\n",
    "        # Uses LeakyReLU and advanced initialization/regularization\n",
    "        encoded = Dense(256, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_1\")(input_layer)\n",
    "        encoded = LeakyReLU(alpha=0.1, name=\"encoder_leakyrelu_1\")(encoded)\n",
    "        encoded = BatchNormalization(name=\"encoder_bn_1\")(encoded)\n",
    "        encoded = Dense(128, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_2\")(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1, name=\"encoder_leakyrelu_2\")(encoded)\n",
    "        encoded = Dropout(0.25, name=\"encoder_dropout_1\")(encoded)\n",
    "        encoded = Dense(64, kernel_initializer=\"he_normal\", name=\"encoder_dense_3\")(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1, name=\"encoder_leakyrelu_3\")(encoded)\n",
    "        \n",
    "        decoded = Dense(64, kernel_initializer=\"he_normal\", name=\"decoder_dense_1\")(encoded)\n",
    "        decoded = LeakyReLU(alpha=0.1, name=\"decoder_leakyrelu_1\")(decoded)\n",
    "        decoded = Dropout(0.25, name=\"decoder_dropout_1\")(decoded)\n",
    "        decoded = Dense(128, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_2\")(decoded)\n",
    "        decoded = LeakyReLU(alpha=0.1, name=\"decoder_leakyrelu_2\")(decoded)\n",
    "        decoded = BatchNormalization(name=\"decoder_bn_1\")(decoded)\n",
    "        decoded = Dense(256, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_3\")(decoded)\n",
    "        decoded = LeakyReLU(alpha=0.1, name=\"decoder_leakyrelu_3\")(decoded)\n",
    "        decoded = Dense(input_dim, activation=\"linear\", name=\"output_layer\")(decoded)\n",
    "        \n",
    "    elif version == \"v5\":\n",
    "        # Deeper model with a bottleneck layer\n",
    "        # Encoder\n",
    "        encoded = Dense(256, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_1\")(input_layer)\n",
    "        encoded = LeakyReLU(alpha=0.1, name=\"encoder_leakyrelu_1\")(encoded)\n",
    "        encoded = BatchNormalization(name=\"encoder_bn_1\")(encoded)\n",
    "        encoded = Dense(128, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"encoder_dense_2\")(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1, name=\"encoder_leakyrelu_2\")(encoded)\n",
    "        encoded = Dropout(0.50, name=\"encoder_dropout_1\")(encoded)\n",
    "        encoded = Dense(64, kernel_initializer=\"he_normal\", name=\"encoder_dense_3\")(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1, name=\"encoder_leakyrelu_3\")(encoded)\n",
    "        \n",
    "        # Bottleneck layer\n",
    "        bottleneck = Dense(64, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"bottleneck\")(encoded)\n",
    "        bottleneck = LeakyReLU(alpha=0.1, name=\"bottleneck_leakyrelu\")(bottleneck)\n",
    "        \n",
    "        # Decoder\n",
    "        decoded = Dense(64, kernel_initializer=\"he_normal\", name=\"decoder_dense_1\")(encoded)\n",
    "        decoded = LeakyReLU(alpha=0.1, name=\"decoder_leakyrelu_1\")(decoded)\n",
    "        decoded = Dropout(0.50, name=\"decoder_dropout_1\")(decoded)\n",
    "        decoded = Dense(128, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_2\")(decoded)\n",
    "        decoded = LeakyReLU(alpha=0.1, name=\"decoder_leakyrelu_2\")(decoded)\n",
    "        decoded = BatchNormalization(name=\"decoder_bn_1\")(decoded)\n",
    "        decoded = Dense(256, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4), name=\"decoder_dense_3\")(decoded)\n",
    "        decoded = LeakyReLU(alpha=0.1, name=\"decoder_leakyrelu_3\")(decoded)\n",
    "        decoded = Dense(input_dim, activation=\"linear\", name=\"output_layer\")(decoded)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown version: {version}\")\n",
    "    \n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded, name=f\"autoencoder_{version}\")\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded, name=f\"encoder_{version}\")\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function ---\n",
    "def train_autoencoder(\n",
    "    model: Model,\n",
    "    X_train: np.ndarray,\n",
    "    X_val: Optional[np.ndarray] = None,\n",
    "    save_path: Optional[Path] = None\n",
    ") -> tf.keras.callbacks.History:\n",
    "    monitor_metric = \"val_loss\" if X_val is not None else \"loss\"\n",
    "    checkpoint_path = save_path or Config.MODEL_DIR / \"best_autoencoder.keras\"\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=monitor_metric, patience=Config.EARLY_STOPPING_PATIENCE, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor=monitor_metric, patience=Config.REDUCE_LR_PATIENCE, factor=Config.REDUCE_LR_FACTOR, min_lr=Config.MIN_LR),\n",
    "        ModelCheckpoint(filepath=checkpoint_path, monitor=monitor_metric, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=Config.LEARNING_RATE), loss=\"mse\")\n",
    "    history = model.fit(\n",
    "        X_train, X_train,\n",
    "        validation_data=(X_val, X_val) if X_val is not None else None,\n",
    "        epochs=Config.EPOCHS,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# --- Helper Functions for Evaluation ---\n",
    "def get_reconstruction_errors(model: Model, X: np.ndarray) -> np.ndarray:\n",
    "    reconstructions = model.predict(X, batch_size=Config.BATCH_SIZE)\n",
    "    return np.mean(np.power(X - reconstructions, 2), axis=1)\n",
    "\n",
    "def evaluate_with_threshold(\n",
    "    model: Model,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    threshold: float\n",
    ") -> Dict[str, Any]:\n",
    "    mse = get_reconstruction_errors(model, X)\n",
    "    y_pred = (mse > threshold).astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"roc_auc\": roc_auc_score(y, mse),\n",
    "        \"threshold\": threshold,\n",
    "        \"confusion_matrix\": confusion_matrix(y, y_pred),\n",
    "        \"classification_report\": classification_report(y, y_pred),\n",
    "        \"average_precision\": average_precision_score(y, mse),\n",
    "        \"f1_score\": f1_score(y, y_pred),\n",
    "        \"precision_score\": precision_score(y, y_pred),\n",
    "        \"recall_score\": recall_score(y, y_pred)\n",
    "    }\n",
    "\n",
    "def select_best_threshold(\n",
    "    model: Model,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    threshold_range: Tuple[float, float] = (0.9, 0.99),\n",
    "    n_jobs: int = -1,\n",
    "    optimize_by: str = \"roc_auc\"\n",
    ") -> float:\n",
    "    mse = get_reconstruction_errors(model, X)\n",
    "    lower_q = np.quantile(mse, threshold_range[0])\n",
    "    upper_q = np.quantile(mse, threshold_range[1])\n",
    "    thresholds = np.linspace(lower_q, upper_q, Config.NUM_THRESHOLDS)\n",
    "    \n",
    "    def evaluate_threshold(t): \n",
    "        metrics = evaluate_with_threshold(model, X, y, t)\n",
    "        return t, metrics.get(optimize_by, 0)\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(evaluate_threshold)(t) for t in thresholds)\n",
    "    best_threshold, best_score = max(results, key=lambda x: x[1])\n",
    "    logger.info(f\"Best threshold selected by {optimize_by}: {best_threshold:.6f} (score: {best_score:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "def plot_loss(\n",
    "    results: Dict[str, Any],\n",
    "    save_path: Path = Config.RESULTS_DIR / \"training_loss.png\"\n",
    ") -> None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(results)))\n",
    "    \n",
    "    for idx, (version, metrics) in enumerate(results.items()):\n",
    "        plt.plot(\n",
    "            metrics['history'].history['loss'],\n",
    "            label=f'{version} Train',\n",
    "            color=colors[idx],\n",
    "            linestyle='-'\n",
    "        )\n",
    "        if 'val_loss' in metrics['history'].history:\n",
    "            plt.plot(\n",
    "                metrics['history'].history['val_loss'],\n",
    "                label=f'{version} Val',\n",
    "                color=colors[idx],\n",
    "                linestyle='--'\n",
    "            )\n",
    "    \n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Loss plot saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training autoencoder version v3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.8228 - val_loss: 0.6618 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.5212 - val_loss: 0.5669 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.4468 - val_loss: 0.5178 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.4070 - val_loss: 0.4840 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.3788 - val_loss: 0.4609 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.3564 - val_loss: 0.4427 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.3397 - val_loss: 0.4297 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.3266 - val_loss: 0.4168 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.3154 - val_loss: 0.4065 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.3063 - val_loss: 0.4002 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2979 - val_loss: 0.3914 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2916 - val_loss: 0.3901 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2850 - val_loss: 0.3821 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2794 - val_loss: 0.3779 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2749 - val_loss: 0.3758 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2702 - val_loss: 0.3709 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2661 - val_loss: 0.3703 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2621 - val_loss: 0.3671 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2583 - val_loss: 0.3652 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2549 - val_loss: 0.3631 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2516 - val_loss: 0.3587 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2486 - val_loss: 0.3618 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2458 - val_loss: 0.3598 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2431 - val_loss: 0.3630 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2401 - val_loss: 0.3582 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2379 - val_loss: 0.3601 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2350 - val_loss: 0.3596 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2331 - val_loss: 0.3586 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2305 - val_loss: 0.3567 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2284 - val_loss: 0.3556 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2263 - val_loss: 0.3565 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2241 - val_loss: 0.3545 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2218 - val_loss: 0.3547 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2203 - val_loss: 0.3573 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001b[1m1615/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2187"
     ]
    }
   ],
   "source": [
    "# --- Main Training & Selection Workflow ---\n",
    "results = {}\n",
    "best_model = None\n",
    "best_f1 = 0.0\n",
    "best_auc = 0.0\n",
    "best_version = None\n",
    "\n",
    "input_dim = X_train_auto.shape[1]\n",
    "\n",
    "for version in [\"v3\"]: # [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\"]\n",
    "    logger.info(f\"Training autoencoder version {version}...\")\n",
    "    autoencoder, encoder = build_autoencoder(input_dim=input_dim, version=version)\n",
    "    history = train_autoencoder(autoencoder, X_train_auto, X_val_auto)\n",
    "    \n",
    "    best_threshold = select_best_threshold(\n",
    "        autoencoder, \n",
    "        X_val_auto, \n",
    "        y_val_auto,\n",
    "        threshold_range=Config.THRESHOLD_RANGE,\n",
    "        optimize_by=Config.OPTIMIZE_BY\n",
    "    )\n",
    "    \n",
    "    val_metrics = evaluate_with_threshold(autoencoder, X_val_auto, y_val_auto, best_threshold)\n",
    "    results[version] = {\n",
    "        \"threshold\": best_threshold,\n",
    "        \"roc_auc\": val_metrics[\"roc_auc\"],\n",
    "        \"f1_score\": val_metrics[\"f1_score\"],\n",
    "        \"precision_score\": val_metrics[\"precision_score\"],\n",
    "        \"recall_score\": val_metrics[\"recall_score\"],\n",
    "        \"classification_report\": val_metrics[\"classification_report\"],\n",
    "        \"val_loss\": min(history.history['val_loss']),\n",
    "        \"history\": history\n",
    "    }\n",
    "    \n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(f\"Version {version} Validation Metrics:\\n{val_metrics['classification_report']}\")\n",
    "    logger.info(f\"ROC-AUC: {val_metrics['roc_auc']:.4f}, F1: {val_metrics['f1_score']:.4f}\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # --- ROC-AUC based model selection ---\n",
    "    if val_metrics[\"roc_auc\"] > best_auc:\n",
    "        best_auc = val_metrics[\"roc_auc\"]\n",
    "        best_f1 = val_metrics[\"f1_score\"]\n",
    "        best_model = autoencoder\n",
    "        best_version = version\n",
    "        logger.info(f\"New best model: {version} (AUC: {best_auc:.4f}, F1: {best_f1:.4f})\")\n",
    "        best_model.save(Config.MODEL_PATH)\n",
    "        with open(Config.THRESHOLD_PATH, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"threshold\": float(best_threshold)\n",
    "            }, f)\n",
    "        with open(Config.CONFIG_PATH, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"input_dim\": input_dim,\n",
    "                \"version\": version,\n",
    "                \"feature_columns\": feature_columns\n",
    "            }, f)\n",
    "\n",
    "logger.info(\"\\n=== Training Complete ===\")\n",
    "logger.info(f\"Best model: {best_model.name if best_model else 'None'}\")\n",
    "logger.info(f\"Validation ROC-AUC: {best_auc:.4f}, F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for all models and returning the saved model and threshold.\n",
    "plot_loss(results)\n",
    "logger.info(f\"Threshold saved: {Config.THRESHOLD_PATH}\")\n",
    "logger.info(f\"Model saved: {Config.MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Hyperparameter Tuning on the Best Autoencoder Model (Efficient Version)\n",
    "\n",
    "This section efficiently tunes the **best saved autoencoder model** using Keras Tuner. The process loads the best model's configuration, including its architecture and previously found optimal L2 regularization and dropout values, and then performs a focused hyperparameter search to further improve validation performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Load Best Model Configuration**\n",
    "\n",
    "- Loads the best model's configuration (`input_dim`, `version`, `l2_reg`, `dropout`) from the saved JSON file.\n",
    "- Ensures the hyperparameter search uses the same architecture and starts from the best-found regularization and dropout values.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Define Efficient HyperModel for Tuning**\n",
    "\n",
    "- **Class:** `AutoencoderHyperModel`\n",
    "- **Purpose:** Builds an autoencoder with the v4 architecture, exposing L2 regularization and dropout rate as tunable hyperparameters.\n",
    "- **Tunable Parameters:**\n",
    "  - **L2 Regularization (`l2_reg`):** Searched on a log scale between `1e-5` and `1e-3`, defaulting to the best previously found value.\n",
    "  - **Dropout Rate (`dropout_rate`):** Searched between `0.1` and `0.7` in steps of `0.05`, defaulting to the best previously found value.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Run Efficient Hyperparameter Search**\n",
    "\n",
    "- **Tuner:** Uses Keras Tuner's `Hyperband` for fast and efficient search.\n",
    "- **Efficiency Improvements:**\n",
    "  - Reduces the maximum number of epochs (e.g., 50) and executions per trial to speed up the search.\n",
    "  - Uses shorter patience for early stopping and learning rate reduction.\n",
    "  - Frees up memory and clears TensorFlow sessions before and after tuning.\n",
    "- **Objective:** Minimizes validation loss (`val_loss`).\n",
    "- **Data:** Uses preprocessed training and validation sets.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Save and Evaluate the Tuned Model**\n",
    "\n",
    "- Saves the best tuned model and logs the optimal hyperparameters.\n",
    "- Finds and saves the best threshold for fraud detection using the tuned model.\n",
    "- Evaluates and logs the tuned model's performance on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "This workflow ensures that hyperparameter tuning is performed efficiently on the best model architecture and regularization settings found during initial training, resulting in a robust and well-regularized final autoencoder for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Efficient Hyperparameter Tuning with Keras Tuner on the Best Saved Model ---\n",
    "# Load best model configuration (input_dim, version, l2_reg, dropout)\n",
    "with open(Config.CONFIG_PATH, \"r\") as f:\n",
    "    best_config = json.load(f)\n",
    "input_dim = best_config[\"input_dim\"]\n",
    "best_version = best_config.get(\"version\", \"v3\")\n",
    "default_l2 = results.get(best_version, {}).get(\"l2_reg\", 1e-4)\n",
    "default_dropout = results.get(best_version, {}).get(\"dropout\", 0.5)\n",
    "\n",
    "class AutoencoderV3HyperModel(HyperModel):\n",
    "    def __init__(self, input_dim, default_l2, default_dropout):\n",
    "        self.input_dim = input_dim\n",
    "        self.default_l2 = default_l2\n",
    "        self.default_dropout = default_dropout\n",
    "\n",
    "    def build(self, hp):\n",
    "        l2_reg = hp.Float('l2_reg', min_value=1e-5, max_value=1e-3, sampling='log', default=self.default_l2)\n",
    "        dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.7, step=0.05, default=self.default_dropout)\n",
    "        input_layer = Input(shape=(self.input_dim,), name=\"input_layer\")\n",
    "        # Encoder\n",
    "        encoded = Dense(256, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(input_layer)\n",
    "        encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "        encoded = BatchNormalization()(encoded)\n",
    "        encoded = Dense(128, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "        encoded = Dropout(dropout_rate)(encoded)\n",
    "        encoded = Dense(64, kernel_initializer=\"he_normal\")(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "        # Bottleneck\n",
    "        bottleneck = Dense(32, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(encoded)\n",
    "        bottleneck = LeakyReLU(alpha=0.1)(bottleneck)\n",
    "        # Decoder\n",
    "        decoded = Dense(64, kernel_initializer=\"he_normal\")(bottleneck)\n",
    "        decoded = LeakyReLU(alpha=0.1)(decoded)\n",
    "        decoded = Dropout(dropout_rate)(decoded)\n",
    "        decoded = Dense(128, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(decoded)\n",
    "        decoded = LeakyReLU(alpha=0.1)(decoded)\n",
    "        decoded = BatchNormalization()(decoded)\n",
    "        decoded = Dense(256, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(decoded)\n",
    "        decoded = LeakyReLU(alpha=0.1)(decoded)\n",
    "        decoded = Dense(self.input_dim, activation='linear', name=\"output_layer\")(decoded)\n",
    "\n",
    "        autoencoder = Model(inputs=input_layer, outputs=decoded, name=\"tuned_autoencoder_v3\")\n",
    "        autoencoder.compile(optimizer=Adam(learning_rate=Config.LEARNING_RATE), loss='mse')\n",
    "        return autoencoder\n",
    "\n",
    "# Free up memory before tuning\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Use a smaller max_epochs and executions_per_trial for faster tuning\n",
    "tuner = Hyperband(\n",
    "    hypermodel=AutoencoderV3HyperModel(input_dim=input_dim, default_l2=default_l2, default_dropout=default_dropout),\n",
    "    objective='val_loss',\n",
    "    max_epochs=min(Config.EPOCHS, 50),  # Reduce epochs for tuning\n",
    "    factor=3,\n",
    "    executions_per_trial=1,  # Only 1 execution per trial for speed\n",
    "    directory=str(Config.MODEL_DIR),\n",
    "    project_name='autoencoder_tuning_v3',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=max(3, Config.EARLY_STOPPING_PATIENCE // 2),  # Shorter patience for tuning\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    factor=0.5,\n",
    "    min_lr=Config.MIN_LR\n",
    ")\n",
    "\n",
    "logger.info(\"Efficient hyperparameter tuning on the best saved model...\")\n",
    "tuner.search(\n",
    "    X_train_auto, X_train_auto,\n",
    "    validation_data=(X_val_auto, X_val_auto),\n",
    "    epochs=min(Config.EPOCHS, 50),\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get and save the best tuned model\n",
    "best_hp_model = tuner.get_best_models(num_models=1)[0]\n",
    "tuned_model_path = Config.MODEL_DIR / 'tuned_autoencoder_v3.keras'\n",
    "best_hp_model.save(tuned_model_path)\n",
    "logger.info(f\"Tuned model saved to {tuned_model_path}\")\n",
    "\n",
    "# Log best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "logger.info(f\"Best hyperparameters (v3):\\n- L2 regularization: {best_hps.get('l2_reg')}\\n- Dropout rate: {best_hps.get('dropout_rate')}\")\n",
    "\n",
    "# Find and save the best threshold for the tuned model\n",
    "best_threshold = select_best_threshold(\n",
    "    best_hp_model,\n",
    "    X_val_auto,\n",
    "    y_val_auto,\n",
    "    threshold_range=Config.THRESHOLD_RANGE,\n",
    "    optimize_by=Config.OPTIMIZE_BY\n",
    ")\n",
    "with open(Config.MODEL_DIR / 'tuned_threshold_v3.json', 'w') as f:\n",
    "    json.dump({'threshold': best_threshold}, f)\n",
    "\n",
    "# Evaluate and log performance\n",
    "val_metrics = evaluate_with_threshold(best_hp_model, X_val_auto, y_val_auto, best_threshold)\n",
    "logger.info(f\"Tuned Model (v3) Metrics:\\n{val_metrics['classification_report']}\")\n",
    "\n",
    "# Clean up after tuning\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Function Documentation: `create_submission_tuned_best_model`\n",
    "\n",
    "This function generates a Kaggle-ready submission file using the best tuned autoencoder model for the IEEE-CIS Fraud Detection task. It ensures that the test data is preprocessed identically to the training data, applies the tuned model and threshold, and saves the results for evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow Steps**\n",
    "\n",
    "1. **Component Loading**\n",
    "   - Loads the trained `StandardScaler` used during model training.\n",
    "   - Loads the best tuned autoencoder model from disk.\n",
    "   - Loads the optimal threshold for fraud classification.\n",
    "\n",
    "2. **Test Data Preprocessing**\n",
    "   - Applies the same preprocessing pipeline to the test data using the loaded scaler.\n",
    "   - Validates that the number of rows in the preprocessed data matches the original test set.\n",
    "\n",
    "3. **Prediction**\n",
    "   - Computes the reconstruction error (mean squared error, MSE) for each test sample using the autoencoder.\n",
    "   - Logs basic statistics about the MSE distribution.\n",
    "\n",
    "4. **Submission Creation**\n",
    "   - Classifies each transaction as fraud (`isFraud = 1`) if its MSE exceeds the threshold, otherwise as non-fraud (`isFraud = 0`).\n",
    "   - Combines `TransactionID` and predicted `isFraud` into a DataFrame.\n",
    "\n",
    "5. **File Saving**\n",
    "   - Saves the submission DataFrame as a CSV file in the results directory.\n",
    "   - Logs the path to the created submission file.\n",
    "\n",
    "6. **Error Handling**\n",
    "   - Logs and handles errors related to missing files or unexpected issues during submission generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Helper Functions**\n",
    "\n",
    "- **`load_scaler(path: Path)`**  \n",
    "  Loads the scaler from disk. Raises an error if the file is missing.\n",
    "\n",
    "- **`preprocess_and_validate(test_df: pd.DataFrame, scaler)`**  \n",
    "  Preprocesses the test data using the provided scaler and checks for row consistency.\n",
    "\n",
    "- **`load_model_safely(path: Path)`**  \n",
    "  Loads the trained Keras model from disk, with error handling.\n",
    "\n",
    "- **`load_threshold(path: Path)`**  \n",
    "  Loads the optimal threshold value from a JSON file.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "This function ensures that the test data is processed and evaluated in a manner consistent with the training pipeline, leverages the best tuned model and threshold, and produces a ready-to-submit CSV file for Kaggle or other evaluation platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_tuned_best_model(test_df: pd.DataFrame) -> None:\n",
    "    logger.info(\"Generating final submission...\")\n",
    "\n",
    "    try:\n",
    "        # Load necessary components\n",
    "        scaler = load_scaler(Config.MODEL_DIR / \"scaler.joblib\")\n",
    "        model = load_model_safely(Config.MODEL_DIR / 'tuned_autoencoder.keras')\n",
    "        version = \"tuned_version\"\n",
    "        \n",
    "        # Preprocess test data\n",
    "        try:\n",
    "            X_test = preprocess_and_validate(test_df, scaler)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Preprocessing failed: {str(e)}\")\n",
    "            return\n",
    "        \n",
    "        # Predict using reconstruction error\n",
    "        mse = get_reconstruction_errors(model, X_test)\n",
    "        logger.info(f\"MSE stats - Min: {mse.min():.2f}, Max: {mse.max():.2f}, Mean: {mse.mean():.2f}\")\n",
    "\n",
    "         # --- Normalize MSE to [0, 1] for probability submission ---\n",
    "        mse_norm = (mse - mse.min()) / (mse.max() - mse.min() + 1e-8)\n",
    "\n",
    "        # Create submission DataFrame with normalized probabilities\n",
    "        submission = pd.DataFrame({\n",
    "            \"TransactionID\": test_df[\"TransactionID\"].astype(str),\n",
    "            \"isFraud\": mse  # Probability for ROC-AUC evaluation\n",
    "        })\n",
    "        \n",
    "        # Save submission\n",
    "        safe_version = version.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        submission_path = Config.RESULTS_DIR / f\"submission_{safe_version}.csv\"\n",
    "        try:\n",
    "            submission.to_csv(submission_path, index=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save submission: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Submission created: {submission_path}\")\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"ValueError: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during submission generation: {str(e)}\")\n",
    "\n",
    "\n",
    "# === Helper Functions ===\n",
    "def load_scaler(path: Path):\n",
    "    if not path.exists():\n",
    "        raise ValueError(\"Scaler file not found. Cannot proceed with submission.\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def preprocess_and_validate(test_df: pd.DataFrame, scaler) -> np.ndarray:\n",
    "    # Load feature columns from config\n",
    "    with open(Config.CONFIG_PATH, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    feature_columns = config.get(\"feature_columns\")\n",
    "    if feature_columns is None:\n",
    "        raise ValueError(\"feature_columns not found in config. Cannot align test data.\")\n",
    "    X_test, _, _ = preprocess_data(test_df, is_train=False, scaler=scaler, feature_columns=feature_columns)\n",
    "    if len(X_test) != len(test_df):\n",
    "        raise ValueError(f\"Data mismatch: {len(X_test)} vs {len(test_df)} rows\")\n",
    "    return X_test\n",
    "\n",
    "def load_model_safely(path: Path):\n",
    "    if not path.exists():\n",
    "        raise ValueError(f\"Model file {path} not found\")\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(path)\n",
    "        logger.info(f\"Loaded model from {path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Model loading failed: {str(e)}\")\n",
    "\n",
    "def load_threshold(path: Path) -> float:\n",
    "    if not path.exists():\n",
    "        raise ValueError(f\"Threshold file {path} missing\")\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)[\"threshold\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Threshold loading failed: {str(e)}\")\n",
    "\n",
    "# Executing the prediction on test data\n",
    "transaction_ids = pd.read_csv(Config.DATA_DIR_PROCESS / 'merged_test.csv')[['TransactionID']]\n",
    "test_data_with_id = test_data.copy()\n",
    "test_data_with_id['TransactionID'] = transaction_ids\n",
    "create_submission_tuned_best_model(test_data_with_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Score Analysis\n",
    "\n",
    "## **Performance Overview**\n",
    "\n",
    "### **Without Hyperparameter Tuning**\n",
    "- **Model Version:** v2\n",
    "- **Scores:**\n",
    "  - **Private Score:** 0.594274  \n",
    "  - **Public Score:** 0.577343  \n",
    "\n",
    "### **With Hyperparameter Tuning**\n",
    "#### **Model Version:** v4\n",
    "- **Scores:**\n",
    "  - **Private Score:** 0.753307 \n",
    "  - **Public Score:** 0.785282  \n",
    "\n",
    "#### **Model Version:** v3\n",
    "- **Scores:**\n",
    "  - **Private Score:** 0.729231  \n",
    "  - **Public Score:** 0.805388  \n",
    "\n",
    "## **Insights**\n",
    "- Hyperparameter tuning significantly improved the model's performance, with **Model v4** achieving the best scores on both private and public leaderboards.\n",
    "- The results highlight the importance of fine-tuning model parameters to optimize performance for fraud detection tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
